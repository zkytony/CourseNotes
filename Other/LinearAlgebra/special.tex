\subsection{Block Matrix}

\begin{definition}[Block Matrix]
A block matrix $\bm{M}$ is defined as
\[\bm{M}=\begin{bmatrix}
    \bm{A} & \bm{B}\\
    \bm{C} & \bm{D}
\end{bmatrix}\]
where $\bm{A}, \bm{B}, \bm{C}, \bm{D}$ are matrices (or block matrices) themselves.
\end{definition}
Block matrices share many useful properties as normal matrices, by treating block entries as normal matrix entries. For example:
\begin{align}
    \bm{M}^2 &=\begin{bmatrix}
        \bm{A} & \bm{B}\\
        \bm{C} & \bm{D}
    \end{bmatrix}\begin{bmatrix}
        \bm{A} & \bm{B}\\
        \bm{C} & \bm{D}
    \end{bmatrix}\\
    &=\begin{bmatrix}
        \bm{A}^2+\bm{BC} & \bm{AB}+\bm{BD}\\
        \bm{CA}+\bm{DC} & \bm{CB}+\bm{D}^2
    \end{bmatrix}
\end{align}


\subsection{Orthogonal}
\begin{definition}[Orthogonal Matrix]
An \emph{orthogonal matrix} $\bm{Q}$ is a square matrix with real entries whose columns and rows are orthogonal unit vectors (i.e., \emph{orthonormal} vectors), i.e.
\begin{equation}\label{eq:orthogonal}
    \bm{Q}^T\bm{Q}=\bm{Q}\bm{Q}^T=\bm{I}
\end{equation}
\end{definition}
Therefore, we have $\bm{Q}^T=\bm{Q}^{-1}$. To fully understand why Equation \ref{eq:orthogonal} holds, we need to know that for two orthogonal vectors $\bm{u_1}$ and $\bm{u_2}$, $\bm{u_1}^T\bm{u_2}=0$. And $\bm{u_1}^T\bm{u_1}=|\bm{u_1}|^2=1$. Therefore, in the resulting matrix, all entries are 0 except for ones along the diagonal.

\subsection{Diagonal}
\begin{definition}[Diagonal Matrix]
A square matrix $\bm{D}$ is a diagonal matrix if all entries except for ones along the main diagonal are 0.
\end{definition}
\underline{Simple fact:} for two diagonal matrices $\bm{D_1}$ and $\bm{D_2}$, their multiplication $\bm{D_1D_2}=\bm{D_3}$ is also a diagonal matrix with each entry $\bm{D_3}[i]$ along\footnote{The $[i]$ just means the $i$th entry along the main diagonal.} the main diagonal equals to $\bm{D_1}[i]\bm{D_2}[i]$.

Therefore, every diagonal matrix is invertible. The inverse $\bm{D}^{-1}$ of diagonal matrix $\bm{D}$ has entries $\bm{D}^{-1}[i]=1/\bm{D}[i]$.

\underline{Another fact:} The determinant of a diagonal matrix is the product of the diagonal entries.

\underline{Yet another fact:} The column vectors of a diagonal matrix $\bm{D}$ are the eigenvectors of $\bm{D}$, and each diagonal entry is the eigenvalue for the eigenvector at the corresponding column, that is
\begin{equation}
    \bm{D}=\begin{bmatrix}
    \lambda_1 &          \\
              & \lambda_2\\
              &          & \ddots\\
              &          &         & \lambda_n
    \end{bmatrix}
\end{equation}
This can be verified simply by solving the characteristic polynomial $det(\bm{D}-\lambda\bm{I})=0$.


\subsection{Diagonalizable}\label{sec:special:diag}
\begin{definition}[Diagonalizable Matrix]
An $n\times n$ matrix $\bm{A}$ is \emph{diagonalizable} if there exists an $n\times n$ matrix $\bm{P}$ such that 
\begin{equation}\label{eq:diag}
    \bm{D}=\bm{P}^{-1}\bm{A}\bm{P}
\end{equation}
where $\bm{D}$ is a diagonal matrix.
\end{definition}

Note that $\bm{D}=\bm{P}^{-1}\bm{A}\bm{P}\Longrightarrow \bm{A}=\bm{P}\bm{D}\bm{P}^{-1}$

\begin{theorem}[The Diagonalization Theorem] ${\ }$

\begin{enumerate}[label=\alph*)]
    \item An $n\times n$ matrix $\bm{A}$ is diagonalizable if and only if $\bm{A}$ has n linearly independent \emph{eigenvectors}.
    \item  $\bm{A}=\bm{PDP}^{-1}$ where $\bm{D}$ is a diagonal matrix \emph{if and only if} all $n$ columns of $\bm{P}$ are linearly independent
eigenvectors of $\bm{A}$ \emph{and} the diagonal entries of $\bm{D}$ are their corresponding eigenvalues.
\end{enumerate}
\end{theorem}
If we can find $n$ linearly independent eigenvectors for an $n\times n$
matrix $\bm{A}$, then we know the matrix is diagonalizable. Furthermore, we can use those eigenvectors and their corresponding eigenvalues to find the invertible matrix $\bm{P}$ and diagonal matrix $\bm{D}$ necessary to show that $\bm{A}$ is diagonalizable.

\begin{theorem}[Power of Diagonalizable Matrix]
If $\bm{A}=\bm{P}\bm{D}\bm{P}^{-1}$, then $\bm{A}^k=\bm{P}\bm{D}^k\bm{P}^{-1}$
\end{theorem}


\subsection{Symmetric}
\begin{definition}[Symmetric Matrix]
A square matrix $\bm{A}$ is symmetric if and only if
\begin{equation}
    \bm{A}=\bm{A}^T
\end{equation}
\end{definition}
For any $n\times m$ matrix $\bm{B}$, the matrix $\bm{B}^T\bm{B}\in\mathbb{R}^{n\times n}$ is symmetric. Also, every square diagonal matrix is symmetric.\\

\noindent\underline{\textit{Facts about symmetric matrix}}
\begin{itemize}
\item Any symmtric matrix:
  \begin{itemize}
    \item has only real eigenvalues;
    \item is always \emph{diagonalizable};
    \item has orthogonal eigenvectors;
  \end{itemize}
\item The symmetric matrix $\bm{A}$ is
  \begin{itemize}
    \item positive definite if all its eigenvalues are positive.
    \item positive semidefinite if all its eigenvalues are non negative..
  \end{itemize}
\end{itemize}


\subsection{Positive-Definite}
We omit the discussion of complex matrices for now.
\begin{definition}[Positive-Definite]
A \emph{symmetric} $n\times n$ real matrix $\bm{A}$ is \emph{positive definite} if for all $\bm{x}\in\mathbb{R}^n \\ \{\bm{0}\}$, 
\begin{equation}
    \bm{x}^T\bm{Ax} > 0
\end{equation}
\end{definition}
 The negative definite, positive semi-definite, and negative semi-definite matrices are defined analogously. For ``$*$ semi-$*$'', zero is allowed (e.g. $\bm{A}$ is positive semi-definite implies $\bm{x}^T\bm{Ax}\geq 0$).
\begin{theorem}
Covariance matrix is positive semi-definite.
\end{theorem}
\noindent Given data $\bm{X}\in\mathbb{n\times d}$, its covariance matrix $\bm{\Sigma}$ is computed by the following:
\begin{align}
    \bm{\Sigma}&=\mathbb{E}[(\bm{X}-\mathbb{E}[\bm{X}])(\bm{X}-\mathbb{E}[\bm{X}])^T]\\
    \intertext{For nonzero $\bm{y}\in\mathbb{R}^d$}
    \bm{y}^T\bm{\Sigma y} &=  \bm{y}^T\mathbb{E}[(\bm{X}-\mathbb{E}[\bm{X}])(\bm{X}-\mathbb{E}[\bm{X}])^T]\bm{y}\\
    &=  \mathbb{E}[\bm{y}^T(\bm{X}-\mathbb{E}[\bm{X}])(\bm{X}-\mathbb{E}[\bm{X}])^T\bm{y}]\\
    &=  \mathbb{E}[\bm{Q}^T\bm{Q}]
\end{align}
For $\bm{Q}=(\bm{X}-\mathbb{E}[\bm{X}])^T\bm{y}$. Therefore, $\bm{y}^T\bm{\Sigma y}\geq 0$, which means $\bm{\Sigma}$ is \emph{positive semi-definite}.


\subsection{Singular Value Decomposition}
\begin{theorem}
  For any given real matrix $\bm{A}\in\mathbb{R}^{n\times m}$, there exists a unique set of matrices $\bm{U}, \bm{S}, \bm{V}$ such that
  \begin{equation}
    \bm{A} = \bm{USV}^T
  \end{equation}
  where $\bm{U}\in\mathbb{R}^{n\times n}$ and $\bm{S}\in\mathbb{R}^{n\times p}$ and $\bm{V}\in\mathbb{R}^{p\times p}$ $\bm{U}^T\bm{U}=\bm{I}$ and $\bm{V}^T\bm{V}=\bm{I}$. This is called the \emph{singular value decomposition} of $\bm{A}$.
\end{theorem}
$\bm{U}$ and $\bm{V}$ are orthonormal matrices. $\bm{S}$ is a diagonal matrix\footnote{More precisely, it is a rectangular diagonal matrix because $n$ may not equal to $p$. Still, $S_{ij}=0$ if $i\neq j$.}. The elements in $\bm{S}$ are called \emph{singular values} of $\bm{A}$. The eigenvectors of $\bm{A}^T\bm{A}$ are columns of $\bm{V}$, and the eigenvectors of $\bm{A}\bm{A}^T$ are columns of $\bm{U}$. The entries in $\bm{S}$ are positive, and sorted in decreasing order ($S_{11}\geq S_{22}\geq\cdots$).

\subsection{Similar}
\begin{definition}[Similar]
  An $n\times n$ matrix $\bm{A}$ is \emph{similar} to an $n\times n$ matrix $\bm{B}$ (denoted as $\bm{A}\sim \bm{B}$) if there exists an invertible matrix $\bm{P}$ such that $\bm{P}^{-1}\bm{A}\bm{P}=\bm{B}$.
\end{definition}

The intuition behind matrix similarity is that the two matrices represent the same linear operator with respect to (possibly) different bases. The matrix $\bm{P}$ can be viewed as a change-of-basis matrix. 

\begin{theorem}
  All similar matrices have the same eigenvalues. \\
\end{theorem}
  \begin{proof}
    Suppose $\bm{A}\sim \bm{B}$ with $\bm{B}=\bm{P}^{-1}\bm{AP}$, and $\bm{B}\bm{x}=\lambda\bm{x}$. Then $\bm{P}^{-1}\bm{APx}=\lambda\bm{x}$. Then $\bm{APx}=\lambda\bm{P}\bm{x}$, which means $\bm{A}$ also has eigenvalue $\lambda$ (but with eigenvector $\bm{Px}$).
  \end{proof}
  \noindent Note that some matrices have the same eigenvalues, but they are not similar.
  
\begin{theorem}[Determine Matrix Similarity\footnote{Reference: \url{http://kom.aau.dk/~jakob/selPubl/papers1995/ijmest_1995.pdf}}]
  Two square matrices are similar if and only if they have the same \emph{Jordan normal form}.
\end{theorem}
%% \begin{corollary}
%%   Two square matrices $\bm{A}$ and $\bm{B}$ are similar only if they have the same eigenvalues, and the algebraic multiplicity of any eigenvalue $\lambda$ for $\bm{A}$ equals the algebraic multiplicity of $\lambda$ for $\bm{B}$.
%% \end{corollary}
%% \begin{corollary}
%%   Two square matrices $\bm{A}$ and $\bm{B}$ are similar only if they have the same eigenvalues, and the geometric multiplicity of any eigenvalue $\lambda$ for $\bm{A}$ equals the geometric multiplicity of $\lambda$ for $\bm{B}$.
%% \end{corollary}
\noindent There are multiple corollaries from this theorem in the reference.



\subsection{Jordan Normal Form}
\begin{definition}[Jordan Normal Form\footnote{Reference: \url{http://mathworld.wolfram.com/JordanCanonicalForm.html}. Jordan normal form is named after French mathematician Camille Jordan.}] The \emph{Jordan normal form} of a linear transformation $T: \mathcal{V}\rightarrow\mathcal{V}$ is a special type of block matrix in which each block consists of Jordan blocks with possibly differing constants $\lambda_i$. In particular, it is a block matrix of the form:
  \begin{align}
    \bm{J}=
    \begin{bmatrix}
    \bm{J_1} & 0        & \cdots & 0\\
    0        & \bm{J_2} & \cdots & 0\\
    \vdots   & \vdots  & \ddots & \vdots\\
    0        & 0  &  \cdots    & \bm{J_p}
    \end{bmatrix}
  \end{align}
  where every $\bm{J_k}$ is a block matrix (Jordan block), defined as:
  \begin{align}
    \bm{J_k}=\begin{bmatrix}
      \lambda_k & 1        & 0         & \cdots & 0         &\\    %    &           &           &   &        &\\
      0        & \lambda_k & 1         & \ddots & 0         &\\    %    &           &           &   &        &\\
      0        & 0         & \lambda_k & \ddots & 0         &\\    %    &           &           &   &        &\\
      \vdots   & \ddots    & \ddots    & \ddots & 1         &\\   %     &           &           &   &        &\\
      0        & 0         & 0         & \cdots & \lambda_k &\\   %     &           &           &   &        &\\
    \end{bmatrix}
  \end{align}
\end{definition}

\subsection{Hermitian}
\begin{definition}
  A \emph{Hermitian matrix} (or self-adjoint matrix) $\bm{A}$ is a complex square matrix that is equal to its own conjugate transpose, namely,
  \begin{align}
    \bm{A}=\bm{A}^H
  \end{align}
\end{definition}
\noindent For properties, refer to \href{https://en.wikipedia.org/wiki/Hermitian_matrix#Properties}{Wikipedia}.
\subsection{Discrete Fourier Transform}
The $N\times N$ DFT matrx is given by $$\bm{F}_N=[e_{nk}]$$ where $n=0,1,\cdots,5$, $k=0,1,\cdots,5$, and
$e_{nk}=e^{-inx_k}=e^{-i2\pi nk/N}$.
Define
\begin{align}
  w=e^{-i2\pi /N}=e^{-i\pi /4}=\frac{(1-i)}{\sqrt{2}}
\end{align}
\begin{align}
  w^{nk}=\Big(\frac{(1-i)}{\sqrt{2}}\Big)^{nk}
\end{align}
The inverse inverse $N\times N$ DFT.
\begin{align}
  \bm{F}_6^{-1}=\overline{\bm{F}_{N}}
\end{align}
where
\begin{align}
  \overline{\bm{F}_{N}}=\frac{1}{N}[\overline{w}^{nk}]
\end{align}
\noindent More details will be on my review of Fourier Series and Fourier Transform.

